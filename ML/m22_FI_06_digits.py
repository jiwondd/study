from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
import numpy as np
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from tensorflow.python.keras.callbacks import EarlyStopping
from sklearn.metrics import r2_score
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MaxAbsScaler,RobustScaler
from tensorflow.keras.utils import to_categorical
import sklearn as sk


#1. 데이터
datasets=load_digits()
x=datasets['data']
y=datasets.target

x = np.delete(x,[0,8,15,16,24,31,32,39],axis=1)
# print(x.shape) #(1797, 64)->(1797, 56)

x_train,x_test,y_train,y_test=train_test_split(x,y,
        train_size=0.8,shuffle=True, random_state=31)

#2. 모델구성
from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from xgboost import XGBClassifier, XGBRegressor

model1=DecisionTreeClassifier()
model2=RandomForestClassifier()
model3=GradientBoostingClassifier()
model4=XGBClassifier()

# 3. 훈련
model1.fit(x_train,y_train)
model2.fit(x_train,y_train)
model3.fit(x_train,y_train)
model4.fit(x_train,y_train)

# 4.평가, 예측
result1=model1.score(x_test,y_test)
print(model1,'의 model.score:',result1)
y_predict1=model1.predict(x_test)
acc1=accuracy_score(y_test,y_predict1)
print(model1,'의 acc_score :',acc1)
print(model1,':',model1.feature_importances_)
print('*************************************************')
result2=model2.score(x_test,y_test)
print(model2,'의 model.score:',result2)
y_predict2=model2.predict(x_test)
acc2=accuracy_score(y_test,y_predict2)
print(model2,'의 acc_score :',acc2)
print(model2,':',model2.feature_importances_)
print('*************************************************')
result3=model3.score(x_test,y_test)
print(model3,'의 model.score:',result3)
y_predict3=model3.predict(x_test)
acc3=accuracy_score(y_test,y_predict3)
print(model3,'의 acc_score :',acc3)
print(model3,':',model3.feature_importances_)
print('*************************************************')
result4=model4.score(x_test,y_test)
print(model4,'의 model.score:',result4)
y_predict4=model4.predict(x_test)
acc4=accuracy_score(y_test,y_predict4)
print(model4,'의 acc_score :',acc4)
print(model4,':',model4.feature_importances_)
print('*************************************************')

# DecisionTreeClassifier() 의 model.score: 0.8333333333333334
# DecisionTreeClassifier() 의 acc_score : 0.8333333333333334
# DecisionTreeClassifier() : [0.         0.01244357 0.0066442  0.00711804 0.00568646 0.07031321
#  0.00185627 0.         0.         0.0014896  0.03271182 0.00860366
#  0.00985297 0.01292129 0.         0.         0.         0.00229308
#  0.01796777 0.03064908 0.04670648 0.09124572 0.00726201 0.
#  0.         0.00061998 0.06427713 0.05493069 0.08240482 0.00712129
#  0.0013922  0.         0.         0.0568057  0.01067447 0.00685554
#  0.00209261 0.01910067 0.00521177 0.         0.         0.00418387
#  0.08076118 0.06220302 0.01912995 0.00372801 0.00638893 0.
#  0.         0.         0.02078146 0.00918034 0.00371684 0.01206267
#  0.02775883 0.         0.         0.         0.00304959 0.00333628
#  0.06239163 0.00283779 0.00123751 0.        ]
# *************************************************
# RandomForestClassifier() 의 model.score: 0.9694444444444444
# RandomForestClassifier() 의 acc_score : 0.9694444444444444
# RandomForestClassifier() : [0.00000000e+00 2.03292375e-03 2.10678989e-02 9.68964733e-03
#  9.63208746e-03 2.28578338e-02 9.50574549e-03 8.93587461e-04
#  5.76705374e-05 1.18316894e-02 2.32075979e-02 6.72643444e-03
#  1.52614634e-02 2.42590567e-02 4.53587414e-03 6.17158303e-04
#  1.44041955e-04 8.26585372e-03 2.00745866e-02 2.61656251e-02
#  3.02415564e-02 5.33195238e-02 8.90954157e-03 1.71021730e-04
#  5.99683201e-05 1.33663866e-02 4.46659865e-02 2.59750510e-02
#  2.80487059e-02 2.08426386e-02 3.00085969e-02 7.65380213e-05
#  0.00000000e+00 3.37358274e-02 2.23830353e-02 2.02650519e-02
#  3.69856649e-02 1.81801350e-02 2.56984047e-02 0.00000000e+00
#  0.00000000e+00 9.53485228e-03 4.10751465e-02 4.62866366e-02
#  2.07100330e-02 1.53934780e-02 2.00646427e-02 9.03092848e-05
#  6.67645707e-05 2.37184956e-03 2.12097372e-02 2.42686571e-02
#  1.25673342e-02 2.53590129e-02 2.78929554e-02 1.81979306e-03
#  2.74938483e-05 1.44360635e-03 2.28341395e-02 9.80895550e-03
#  2.31136958e-02 2.56684280e-02 1.55539631e-02 3.07810441e-03]
# *************************************************
# GradientBoostingClassifier() 의 model.score: 0.9638888888888889
# GradientBoostingClassifier() 의 acc_score : 0.9638888888888889
# GradientBoostingClassifier() : [0.00000000e+00 1.35222017e-03 4.79621454e-03 2.91403968e-03
#  2.37566474e-03 6.53355455e-02 7.29467281e-03 7.48190493e-04
#  8.05379204e-04 1.12001456e-03 1.86270877e-02 4.31695579e-04
#  6.39428716e-03 1.00255142e-02 2.40558330e-03 4.09415162e-04
#  4.47474999e-04 2.78278470e-03 1.78612088e-02 3.58947994e-02
#  2.87953930e-02 9.26464274e-02 5.18648094e-03 1.19941781e-07
#  9.58757208e-07 1.16420701e-03 4.77260434e-02 1.25566336e-02
#  3.44914022e-02 2.13547368e-02 7.15299145e-03 4.04539800e-04
#  0.00000000e+00 6.20984941e-02 7.28914151e-03 3.23207059e-03
#  6.43135606e-02 1.00701969e-02 1.63287621e-02 0.00000000e+00
#  0.00000000e+00 3.95547589e-03 9.10877791e-02 7.33702293e-02
#  5.12618832e-03 2.80688990e-02 1.52952295e-02 3.32587601e-04
#  2.84743047e-09 6.37687468e-04 1.24668835e-02 1.92987862e-02
#  1.03037104e-02 1.42880587e-02 2.55402803e-02 1.60344206e-05
#  6.46856028e-04 4.22944127e-04 1.56144292e-02 1.64690034e-03
#  4.92730584e-02 6.20113999e-03 1.89474814e-02 1.06254053e-02]
# *************************************************
# XGBClassifier의 model.score: 0.9638888888888889
# XGBClassifier의 acc_score : 0.9638888888888889
# XGBClassifier: [0.         0.05241993 0.00779986 0.0069562  0.00671833 0.03866356
#  0.0081174  0.02513507 0.         0.0091487  0.01149325 0.00390987
#  0.005657   0.01293386 0.00473345 0.         0.         0.00692248
#  0.00837418 0.04150823 0.01411773 0.05266297 0.00394046 0.01945919
#  0.         0.00267471 0.0316786  0.01119414 0.02756132 0.01636295
#  0.00935594 0.         0.         0.06371126 0.00822085 0.0102473
#  0.05539686 0.01168689 0.02595896 0.         0.         0.00700066
#  0.03818741 0.04400676 0.01666461 0.02303649 0.02612244 0.
#  0.         0.00320136 0.0044535  0.00744466 0.01118895 0.01586658
#  0.02550101 0.00081935 0.         0.00275824 0.01058489 0.00257802
#  0.06492653 0.013354   0.03156123 0.03602184]


# [0,8,15,16,24,31,32,39] 8개 컬럼삭제 / 전반적으로 조금씩 좋아지긴 함
# DecisionTreeClassifier() 의 model.score: 0.8444444444444444
# DecisionTreeClassifier() 의 acc_score : 0.8444444444444444
# DecisionTreeClassifier() : [0.         0.0058063  0.00879384 0.00593691 0.08012308 0.
#  0.         0.0064057  0.03125991 0.00510474 0.0100003  0.0068321
#  0.         0.00371106 0.01504157 0.0283803  0.04471577 0.08581502
#  0.00726201 0.00150269 0.00495128 0.06619028 0.05694165 0.08092121
#  0.00382504 0.         0.0564337  0.0141045  0.00514894 0.00739558
#  0.01878751 0.00587677 0.         0.00418387 0.08934609 0.06220302
#  0.01572679 0.00225846 0.00638893 0.         0.         0.
#  0.02072989 0.00593923 0.00857021 0.01206267 0.02775883 0.
#  0.         0.         0.01212763 0.00320427 0.05784764 0.00283779
#  0.00154689 0.        ]
# *************************************************
# RandomForestClassifier() 의 model.score: 0.9777777777777777
# RandomForestClassifier() 의 acc_score : 0.9777777777777777
# RandomForestClassifier() : [1.39463199e-03 1.94264838e-02 1.12864612e-02 1.02230184e-02
#  2.06296861e-02 8.17094777e-03 7.71716702e-04 9.59294266e-03
#  2.21225700e-02 7.67700917e-03 1.40236183e-02 2.77706724e-02
#  4.92677270e-03 7.83694765e-03 2.15315101e-02 2.65030470e-02
#  3.44946187e-02 4.67135676e-02 1.10225975e-02 6.61411361e-04
#  1.25247068e-02 4.04345360e-02 2.77134300e-02 3.02125091e-02
#  2.44950528e-02 3.28627387e-02 2.77418069e-02 3.11126642e-02
#  1.61002452e-02 3.46625958e-02 1.94926113e-02 2.13262005e-02
#  4.11917043e-05 1.10252329e-02 3.93748813e-02 4.72576458e-02
#  1.78576189e-02 1.80941733e-02 2.12691933e-02 5.48794464e-05
#  0.00000000e+00 2.14962900e-03 2.05903344e-02 2.03572521e-02
#  1.48261772e-02 2.11253725e-02 2.51191324e-02 1.24374852e-03
#  5.73177111e-05 2.10935220e-03 2.64542585e-02 9.19771297e-03
#  2.61212464e-02 3.00250573e-02 1.69218393e-02 3.26742451e-03]
# *************************************************
# GradientBoostingClassifier() 의 model.score: 0.9694444444444444
# GradientBoostingClassifier() 의 acc_score : 0.9694444444444444
# GradientBoostingClassifier() : [1.45446015e-03 4.23652082e-03 3.15715482e-03 2.52905827e-03  
#  6.52755731e-02 7.74701265e-03 1.19675468e-03 1.44525184e-03
#  1.85932649e-02 3.83755741e-04 6.38952383e-03 1.05912129e-02
#  2.41884825e-03 2.73809104e-03 1.78605336e-02 3.67997328e-02
#  2.91140870e-02 9.23827895e-02 5.48308183e-03 3.99135637e-08
#  9.16127261e-04 4.74607327e-02 1.24359532e-02 3.48945774e-02
#  2.13521012e-02 7.77513093e-03 6.21070634e-02 7.19878087e-03
#  3.24157393e-03 6.45308475e-02 9.36620302e-03 1.69899729e-02
#  0.00000000e+00 3.85032897e-03 9.07908151e-02 7.37861299e-02
#  5.40233200e-03 2.83670689e-02 1.53050009e-02 2.70560112e-05
#  2.42475231e-06 6.92179774e-04 1.25506394e-02 1.84528104e-02
#  1.05400187e-02 1.35065052e-02 2.52501981e-02 2.12672462e-04
#  6.47956751e-04 4.40937250e-04 1.58339070e-02 1.48906397e-03
#  4.97366280e-02 5.79204999e-03 1.88303061e-02 1.04271583e-02]
# *************************************************
# XGBClassifier의 model.score: 0.9638888888888889
# XGBClassifier의 acc_score : 0.9638888888888889
# XGBClassifier: [0.05241993 0.00779986 0.0069562  0.00671833 0.03866356 0.0081174
#  0.02513507 0.0091487  0.01149325 0.00390987 0.005657   0.01293386
#  0.00473345 0.00692248 0.00837418 0.04150823 0.01411773 0.05266297
#  0.00394046 0.01945919 0.00267471 0.0316786  0.01119414 0.02756132
#  0.01636295 0.00935594 0.06371126 0.00822085 0.0102473  0.05539686
#  0.01168689 0.02595896 0.         0.00700066 0.03818741 0.04400676
#  0.01666461 0.02303649 0.02612244 0.         0.         0.00320136
#  0.0044535  0.00744466 0.01118895 0.01586658 0.02550101 0.00081935
#  0.         0.00275824 0.01058489 0.00257802 0.06492653 0.013354
#  0.03156123 0.03602184]